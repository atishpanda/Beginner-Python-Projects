{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping GitHub Topics Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a website and describe your objective\n",
    "- Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "- Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outline\n",
    "- Scrape website &#8594; https://datastackjobs.com\n",
    "- We will scrape the following content\n",
    "    - Company Name\n",
    "    - Job Title\n",
    "    - Location\n",
    "    - Tags or Skills\n",
    "    - Time Posted\n",
    "    - Job Type\n",
    "    - Category\n",
    "- We will then arrange the data in a tabular form and eport it to  CSV file\n",
    "\n",
    "- ![datastack jobs](https://beeimg.com/images/i82872082642.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website --> https://datastackjobs.com/ has returned a response having status code --> 200.\n",
      "Read more about status code --> https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\U0001f4cd' in position 34844: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Personal Project Work\\Beginner-Python-Projects\\Projects\\GitHub-WebScrapping.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Project%20Work/Beginner-Python-Projects/Projects/GitHub-WebScrapping.ipynb#ch0000003?line=20'>21</a>\u001b[0m \u001b[39m#  parsing the content returned by response\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Project%20Work/Beginner-Python-Projects/Projects/GitHub-WebScrapping.ipynb#ch0000003?line=21'>22</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Project%20Work/Beginner-Python-Projects/Projects/GitHub-WebScrapping.ipynb#ch0000003?line=22'>23</a>\u001b[0m \u001b[39m# savinf the response content in a html file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Personal%20Project%20Work/Beginner-Python-Projects/Projects/GitHub-WebScrapping.ipynb#ch0000003?line=23'>24</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mWeb_Scrapping_content.html\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Personal%20Project%20Work/Beginner-Python-Projects/Projects/GitHub-WebScrapping.ipynb#ch0000003?line=24'>25</a>\u001b[0m     file\u001b[39m.\u001b[39;49mwrite(response\u001b[39m.\u001b[39;49mtext)\n",
      "File \u001b[1;32mc:\\Users\\atish.panda\\AppData\\Local\\Programs\\Python\\Python38\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/atish.panda/AppData/Local/Programs/Python/Python38/lib/encodings/cp1252.py?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> <a href='file:///c%3A/Users/atish.panda/AppData/Local/Programs/Python/Python38/lib/encodings/cp1252.py?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_encode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,encoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\U0001f4cd' in position 34844: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\"\"\"Import libraries if not then install them\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import requests # to work with http and api requests\n",
    "import bs4 # for parsing HTML, XML, JSON and other data\n",
    "\n",
    "# Config File\n",
    "config = {}\n",
    "config[\"base_url\"] = r\"https://datastackjobs.com\"\n",
    "config[\"status_code_info_url\"] = r\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\"\n",
    "\n",
    "# using requests library to fetch data from base_url\n",
    "response = requests.get(config[\"base_url\"])\n",
    "\n",
    "# check for response and status code of response\n",
    "print(f\"Website --> {response.url} has returned a response having status code --> {response.status_code}.\\\n",
    "\\nRead more about status code --> {config['status_code_info_url']}\")\n",
    "\n",
    "#  parsing the content returned by response\n",
    "\n",
    "# savinf the response content in a html file\n",
    "with open('Web_Scrapping_content.html', 'w') as file:\n",
    "    file.write(response.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the requests library to download web pages\n",
    "\n",
    "- Inspect the website's HTML source and identify the right URLs to download.\n",
    "- Download and save web pages locally using the requests library.\n",
    "- Create a function to automate downloading for different topics/search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Beautiful Soup to parse and extract information\n",
    "\n",
    "- Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
    "- Use the right properties and methods to extract the required information.\n",
    "- Create functions to extract from the page into lists and dictionaries.\n",
    "- (Optional) Use a REST API to acquire additional information if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV file(s) with the extracted information\n",
    "\n",
    "- Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
    "- Execute the function with different inputs to create a dataset of CSV files.\n",
    "- Verify the information in the CSV files by reading them back using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document and share your work\n",
    "\n",
    "- Add proper headings and documentation in your Jupyter notebook.\n",
    "- Publish your Jupyter notebook to your Jovian profile\n",
    "- (Optional) Write a blog post about your project and share it online.\n",
    "\n",
    "**Credit and Source** --> [**Jovian - Building a Python Web Scraping Project From Scratch**](https://jovian.ai/aakashns/python-web-scraping-project-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5cf268439eddf61ad37bad768a6371fa88c0de579168edb65ec6bbe41af8c40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
